---
# layout: post
title: "[DL] 왜 Softmax Function을 사용하는가"
categories:
 - ML/DL
tags: 
 - DL
 - Softmax
 - activation function
# share: true 
comments: true 
toc: true
toc_label: 'Contents'
toc_sticky: true
read_time: false
mathjax: true
---

<style>
  table, tr, td {
    /* border-collapse: collapse; */
    /* border: solid 1px black; */
    text-align: center;
    width: 60%;
    margin: 0 auto;
    padding: 5px;
  }
</style>

<style type="text/css">
    img {
        width: 500px;
        text-align: center;
    }
</style>

softmax function에 대해 가지게 된 궁금증에 대한 글입니다. 구글링하며 찾게 된 질문 및 답변을 필자가 이해하기 쉽게 정리하였습니다.  

$$softmax(z)_i=\frac{exp(z_i)}{\sum_j exp(z_j)}$$

- softmax는 differentiable하며 0과 1 사이의 값으로 normalize 해준다는 특성이 있습니다.  

## Why use softmax as opposed to standard normalization
- [원글](https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization)은 여기서 확인하실 수 있습니다.
- 해당 글은 **왜 Z-normalization 대신 softmax를 사용해서 output distribution을 뽑아내는가**에 관한 질문입니다.  
    1. **output distribution의 차이를 더 크게 만들어준다.**
        - image를 예를 들어 설명하자면, softmax function은 각 픽셀들이 거의 uniform distribution을 띄고 있는 low stimulation (blurry image)과, 0 또는 1에 가까운 값들을 가지는 high stimulation (crisp image)에 모두 잘 반응합니다.
        - 다음과 같은 예를 보면, z-normalization을 사용할 때보다 softmax를 사용할 경우, output distribution이 조금 더 명확하게, 구분되어 나올 수 있도록 만들어주는 역할을 합니다. (이는 다른 trick들, 예를 들어 pooling이나 optimizer를 사용하는 이유와 같습니다.)

            ````
            >>> std_norm([1,2])                      # blurry image of a ferret
            [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?
            >>> softmax([1,2])              # blurry image of a ferret
            [0.26894142,      0.73105858])  #     it is a cat perhaps !?


            >>> std_norm([10,20])                    # crisp image of a cat
            [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?
            >>> softmax([10,20])            # crisp image of a cat
            [0.0000453978687, 0.999954602]) #     it is definitely a CAT !
            ````

    1. **cross-entropy loss의 log를 상쇄하여 더 쉽게 계산할 수 있다.**
        - neural network를 학습하는 방법 중 가장 흔히 쓰이는 방법이 Maximum Likelihood Estimator (MLE)인데, 각 sample의 likelihood를 곱하는 것보다 아래와 같이 log를 씌워 더하는 것이 더 간편하다.  
        - *k*번째 sample이라고 했을 때, 
        $$argmax_{\theta} \sum_{k=1}^m log(P(y^{(k)}|x^{(k)}; \theta)) \cdots (1)$$  
        - 이 때, softmax를 사용하면 위의 계산이 $z_i$의 형태로 나오게 되고, 이는 결국 모델이 잘못되더라도 쉽게 수정할 수 있다.
        - true class *i*에 대해 
        $$P(y^{(k)}|x^{(k)}; \theta)=P(y^{(k)}|z)=softmax(z)_i \cdots (2)$$
        - 식 (1)의 $log(softmax(z)_i)$을 구하기 위해 식 (2)에 log를 씌우면 
        $$ log(softmax(z)_i) =  (z_i-log \sum_j exp(z_j)) \cdots (3)$$
        - z의 class 간의 차이가 크다면, linear한 형태로 나타난다.
        $$ log(softmax(z)_i) =  (z_i-\max_jz_j)$$
            - 여기서, 차이가 크다는 것은  
            Let $M = max(z_j)$, then $z_i - log∑exp(z_j) \\ = z_i - log∑exp(M + z_j - M)$  
            $ = z_i - log∑(exp(M) * exp(z_j - M)) \\ = z_i - M + log∑exp(z_j - M)$.  
            When there are large differences in z,  
            we can approximate $exp(z_j - M) ≈ 0 $ for $z_j ≠ M \cdots (4)$.  
            So $z_i - M + log∑exp(z_j - M) ≈ z_i - M + log(exp(M - M)) = z_i - M$ 
        - 모델이 정확하면 $max(z) = x_i$가 되어, 식 (3)의 log-likelihood가 0에 수렴한다.
        - 그러나 모델이 틀리면 $z_j$가 $z_i$보다 크므로 식 (4)가 성립하지 않게 되고, 따라서 log-likelihood는 $z_i-z_j$가 된다. 그렇다면 모델은 $z_i$를 증가시키고 $z_j$를 감소시키는 방향으로 학습을 하게 된다.

